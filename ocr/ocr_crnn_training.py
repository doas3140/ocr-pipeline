# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05_ocr_crnn_training.ipynb (unless otherwise specified).

__all__ = ['PAD', 'PAD', 'DATA_PATH', 'crnn_config', 'allowed_chars', 'allowed_fonts', 'TextlineProcessor',
           'TextlineAndFont', 'one_hot_text', 'decode_single_ctc', 'decode_ctc', 'TextlineList', 'im2seq_data_collate',
           'str2lines', 'MyImageList', 'gaussian_blur', 'resize_tfm', 'rand_resize', 'resize_one_img',
           'train_transforms', 'valid_transforms', 'normalize_images', 'denormalize_images', 'opencv_transform_images',
           'threshold_image', 'create_data', 'conv_output', 'CNN', 'RevConv', 'get_normal_cnn', 'get_partially_rev_cnn',
           'CRNN', 'image_width2seq_len', 'CTCFontLoss', 'AddLossMetrics', 'wer', 'word_error', 'char_error',
           'decode_true', 'WordErrorRate']

# Cell
from fastai import *
from fastai.vision import *
import pandas as pd
import numpy as np
import cv2
from tqdm.notebook import tqdm

# Cell
from .core import save_inference, load_inference
from .ocr_dataset_fontsynth import create_df as create_fontsynth_df
from .ocr_dataset_sroie2019 import create_df as create_sroie_df
from .ocr_dataset_brno import create_df as create_brno_df
from .ocr_dataset_sroie2019 import sroie_ocr_config, DATA_PATH, char_freq
from .ocr_dataset_fontsynth import fontsynth_config, char_freq
from .ocr_dataset_brno import brno_ocr_config
PAD = sroie_ocr_config.PAD # PAD - how much is data padded
PAD = 0
DATA_PATH = fontsynth_config.LINES_DIR

# Cell
allowed_chars = {'N', '3', 'V', 'P', '7', '1', '#', '9', '"', 'C', 'Q', 'B', 'E', '>', '@', ',', 'M', '{', ']',
                 ';', '^', "'", '&', '6', 'Z', '*', '<', '+', 'G', 'X', '!', ':', '-', '[', '|', '$', '5', 'I',
                 'H', '=', 'Y', '.', 'R', 'S', '/', 'T', '}', 'K', '0', '?', 'U', ')', '_', 'D', 'J', 'L', '4',
                 'W', '%', '(', ' ', 'F', '8', '~', '\\', 'A', '2', 'O'}

# allowed_chars = fontsynth_config.allowed_chars

allowed_fonts = ['Unknown', 'Andale_Mono', 'Arial', 'Arial_Black', 'Arial_Bold', 'Arial_Bold_Italic', 'Arial_Italic',
'Comic_Sans_MS_Bold', 'Courier_New', 'Courier_New_Bold', 'Courier_New_Bold_Italic', 'Courier_New_Italic',
'Georgia', 'Georgia_Bold', 'Georgia_Bold_Italic', 'Georgia_Italic', 'Impact', 'Times_New_Roman',
'Times_New_Roman_Bold', 'Times_New_Roman_Bold_Italic', 'Times_New_Roman_Italic', 'Trebuchet_MS',
'Trebuchet_MS_Bold', 'Trebuchet_MS_Bold_Italic', 'Trebuchet_MS_Italic', 'Verdana', 'Verdana_Bold',
'Verdana_Bold_Italic', 'Verdana_Italic', 'brno_easy', 'brno_medium', 'sroie2019', 'Comic_Sans_MS']

class crnn_config:
    LINE_HEIGHT = 48
    USE_DEFAULT_CLASSES = True
    label_delim = '`'
    pad_idx = 0 # aka: label_delim idx
    allowed_chars = allowed_chars
    allowed_fonts = allowed_fonts

# Cell
# label_delim = '`' # '<pad>''

class TextlineProcessor(PreProcessor):
    "`PreProcessor` that create `classes` from `ds.items` and handle the mapping."
    def __init__(self, ds:ItemList):
        self.create_classes(ds.classes, ds.font_classes)
        self.use_default_classes = crnn_config.USE_DEFAULT_CLASSES
        self.default_classes = crnn_config.allowed_chars
        self.default_font_classes = crnn_config.allowed_fonts

    # optional
    def create_classes(self, classes, font_classes):
        self.classes, self.font_classes = classes, font_classes
        if classes is not None:
            self.classes = [crnn_config.label_delim] + classes
            self.c2i = {v:k for k,v in enumerate(self.classes)}
            self.f2i = {v:k for k,v in enumerate(font_classes)}

    def process_one(self,item):
        string, font = item
        return [ self.c2i[c] for c in string ], self.f2i[font]

    def process(self, ds):
        if self.classes is None: self.create_classes(*self.generate_classes(ds.items))
        ds.classes = self.classes
        ds.c2i = self.c2i
        ds.font_classes = self.font_classes
        ds.f2i = self.f2i
        super().process(ds)

    # optional
    def generate_classes(self, items):
        if self.use_default_classes:
            classes = list(self.default_classes)
            font_classes = list(self.default_font_classes)
        else:
            classes, font_classes = set(), set()
            for c,font in items:
                classes = classes.union(set(c))
                font_classes.add(font)
            classes, font_classes = list(classes), list(font_classes)
        classes.sort(); font_classes.sort()
        return classes, font_classes

# Cell
class TextlineAndFont(ItemBase):
    ''' F = font, S = string
    data: tensor(S), tensor(F)
    obj: str(S), str(F)
    raw: str(S), list(F)
    '''
    def __init__(self, data, obj, raw):self.data, self.obj, self.raw = data, obj, raw
    def __str__(self, n=20):
        string = self.obj[0][:n]+['...'] if len(self.obj[0]) > n else self.obj[0]
        return self.obj[1][:5] +'...'+ crnn_config.label_delim.join([str(o) for o in string])
    def __hash__(self): return hash(str(self))

# Cell
def one_hot_text(x:Collection[int], c:int):
    "One-hot encode `x` with `c` classes."
    ''' x w/ len of n returns [n,c] shape arr '''
    res = np.zeros((len(x),c), np.float32)
    res[np.arange(len(x)), listify(x)] = 1.
    return res

# Cell
def decode_single_ctc(t, blank_char=0): # [s_e] -> [s_d], where s_d < s_e
    char_list = []
    for i in range(len(t)):
        if t[i] != blank_char and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank.
            char_list.append(t[i])
    return char_list

def decode_ctc(texts, blank_char=0): # [b,s_e] -> [b,s_d], where s_d < s_e
    return [tensor(decode_single_ctc(t, blank_char=blank_char)) for t in texts ]

# Cell
class TextlineList(ItemList):
    _processor = TextlineProcessor
    def __init__(self, items:Iterator, classes=None, font_classes=None, label_delim:str=None, one_hot:bool=False, **kwargs):
        self.classes = classes
        self.font_classes = font_classes
        items = [(string.split(crnn_config.label_delim),font) for string,font in items] # CHANGED
        super().__init__(items, **kwargs)
        self.processor = [TextlineProcessor(self)]

    def get(self, i):
        stridxs, fontidx = self.items[i] # int, list of ints
        return TextlineAndFont( (tensor(stridxs), tensor(fontidx)),
                                ([self.classes[c] for c in stridxs], self.font_classes[fontidx]), self.items[i])

    def analyze_pred(self, nn_output, thresh=0.5, _=None):
        font_pred, y_pred = nn_output # [c1], [s_e,c2]
        assert len(listify(y_pred.shape)) == 2 # (no batch inputs)
        return font_pred.argmax(dim=-1), decode_single_ctc(y_pred.argmax(dim=-1)), _, _ # [1], [seq_len], _, _

    def reconstruct(self, data_out):
        fontidx, t_argmax, _, lengths = data_out # output from data / output from nn_out -> analyze_pred
        stridxs = [int(i) for i in t_argmax]
        fontidx = int(fontidx)
        return TextlineAndFont((one_hot_text(stridxs, self.c), fontidx),
                                ([self.classes[c] for c in stridxs], self.font_classes[fontidx]), data_out)

    @property
    def c(self): return len(self.classes)

# Cell
def im2seq_data_collate(batch:ItemsList, pad_idx:int=0)->Tensor:
    if isinstance(batch[0][1], int): return data_collate(batch)
    "Convert `batch` items to tensor data."
    data = to_data(batch) # list of (image, text) pairs
    # image: [3,48,w], text: [n,c], where n's and w's are different
    max_w = max([image.shape[2] for image, (text,font) in data])
    max_h = max([image.shape[1] for image, (text,font) in data])
    max_n = max([text.shape[0] for image, (text,font) in data])
#     _, num_classes = data[0][1].shape

    images = torch.zeros(len(batch), 3, max_h, max_w)
    fonts = torch.zeros(len(batch)).long()
#     texts = torch.zeros(len(batch), max_n, num_classes)
    texts = []
    nn_out_seq_len, texts_len = [], []
    for i, (image, (text,font)) in enumerate(data):
        fonts[i] = font
        c,h,w = image.shape
        images[i, : , : , :w ] = image
        images[i, : , : , w: ] = image[:,:,w-1].unsqueeze(2).expand(c,h,max_w-w)
        nn_out_seq_len.append( image_width2seq_len(w) )
        n = text.size(0)
        texts.append( tensor(text) )
#         texts[i, :n , : ] = tensor(text)
#         texts[i, n: , -1 ] = 1
        texts_len.append(n)
#     texts = torch.cat(texts, axis=0)
    return images, (fonts, texts, tensor(nn_out_seq_len).type(torch.int), tensor(texts_len).type(torch.int))

# Cell
def str2lines(string, n=50):
    return ''.join([s+'\n' if (i+1)%n == 0 else s for i,s in enumerate(string)])

str2lines('asdasdasdasdasdasdasdasdasdasdasdasdasdasdasdasdasdasdasdasdasdasdasdasd')

# Cell
class MyImageList(ImageList):
    def show_xys(self, xs, ys, imgsize:int=4, figsize:Optional[Tuple[int,int]]=None, **kwargs):
        "Show the `xs` (inputs) and `ys` (targets) on a figure of `figsize`."
        rows = int(np.ceil(math.sqrt(len(xs))))
        axs = subplots(rows, 1, imgsize=imgsize, figsize=figsize) # CHANGED rows -> 1
        for x,y,ax in zip(xs, ys, axs.flatten()): x.show(ax=ax, y=y, **kwargs)
        for ax in axs.flatten()[len(xs):]: ax.axis('off')
        plt.tight_layout()

    def show_xyzs(self, xs, ys, zs, imgsize:int=10, figsize:Optional[Tuple[int,int]]=None, **kwargs):
        "Show `xs` (inputs), `ys` (targets) and `zs` (predictions) on a figure of `figsize`."
        title = 'Ground truth\nPredictions'
        rows = int(np.ceil(math.sqrt(len(xs))))
        axs = subplots(rows, 1, imgsize=imgsize, figsize=figsize, title=title, weight='bold', size=12).flatten()
        for x,y,z,ax in zip(xs,ys,zs,axs):
            x.show(ax=ax, title=f'y_true: {str2lines(str(y))}\ny_pred: {str2lines(str(z))}', **kwargs)
#         for ax in axs.flatten()[len(xs):]: ax.axis('off')

# Cell
def _gaussian_blur(x, size:uniform_int):
    blurred = cv2.blur(image2np(x), (size,size)) # np.arr
#     blurred = cv2.GaussianBlur(image2np(x), (size,size), 0)
    return tensor(blurred).permute(2,0,1)

def gaussian_blur(size, p=1.0):
    return RandTransform(tfm=TfmPixel(_gaussian_blur), kwargs={'size':size}, p=p, resolved={}, do_run=True, is_random=True, use_on_y=False)

# Cell
resize_one_img = lambda x, size: F.interpolate(x[None], size=size, mode='bilinear', align_corners=True)[0]

def resize_tfm(x, pad:uniform_int, line_height=crnn_config.LINE_HEIGHT):
    ''' size of subtracted padding '''
    c,h,w = x.shape
    x = x[ : , pad:h-pad , pad:w-pad ]
    new_w = int(w * line_height / float(h))
    return resize_one_img(x, size=(line_height, new_w))

def rand_resize(pad, p=1.0):
    return RandTransform(tfm=TfmPixel(resize_tfm), kwargs={'pad':pad}, p=p, resolved={}, do_run=True, is_random=True, use_on_y=False)

# Cell
train_transforms = [
    rand_resize(pad=(0,PAD), p=1.0),
    rotate(degrees=(-3, 3), p=0.6),
    symmetric_warp(magnitude=(-0.03, 0.03), p=0.1),
    rand_zoom(scale=(0.9,1.03), p=0.5),
    brightness(change=(0.35, 0.65), p=0.4),
    contrast(scale=(0.7,1.3), p=0.4),
    gaussian_blur(size=(1, 7), p=0.2),
#     squish(scale=(0.85,1.15), p=0.3),
#     cutout(n_holes=(0,6), length=(1,10)), # black rect
#     tilt(direction=(0,3), magnitude=(-0.2,0.2), p=0.3)
]

valid_transforms = [
    rand_resize(pad=(0,0), p=1.0) # (no padding, but need to resize)
]

# Cell
def normalize_images(ims):
    _min = ims.min()
    ims = ims - _min
    _max = ims.max()
    return ims/_max, _min, _max

def denormalize_images(ims, _min=None, _max=None):
    return (ims * _max) + _min

# Cell
def opencv_transform_images(im_fun):
    def transform(ims, **kwargs):
        device, dtype = ims.device, ims.dtype
        ims, _min, _max = normalize_images(ims)
        out_ims = []
        for im in (ims*255.).long():
#             plot(image2np(im))
            im = im_fun(image2np(im).astype(np.uint8), **kwargs)
#             plot(im)
            out_ims.append( tensor(im).permute(2,0,1)[None] )
        ims = torch.cat(out_ims, dim=0) / 255.
#         ims = normalize_images(ims)[0]
        ims = denormalize_images(ims, _min, _max)
        return ims.to(device=device, dtype=dtype)
    return transform

def threshold_image(im_orig): # [h,w,3]
    im_grey = cv2.cvtColor(im_orig, cv2.COLOR_BGR2GRAY)
    _,th = cv2.threshold(im_grey,0,1,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU) # [h,w]
    mask = cv2.dilate(th, (5,5), iterations=3).astype(bool)
    out = np.zeros_like(im_orig) + 255
    text_im,_,_ = normalize_images(im_orig[mask].astype(float))
    out[mask] = (text_im*255.).astype(np.uint8)
    return out

# Cell
def create_data(df, bs=32):
    ''' DataFrame (df) -> Dataloader (dl) '''
    data = (MyImageList.from_df(df, path='.', cols='image_path')
        .split_from_df(col='valid')
        .label_from_df(cols='string', label_cls=TextlineList, label_delim=crnn_config.label_delim)
        .transform((train_transforms, valid_transforms), tfm_y=False)
        .databunch(bs=bs, collate_fn=partial(im2seq_data_collate, pad_idx=crnn_config.pad_idx))
        .normalize()
    )

    def preprocessing(b):
        x,y = b
        x = opencv_transform_images(threshold_image)(x)
#         x = opencv_transform_images(lambda im: normalize_images(im)[0]*255)(x)
        return x,y
#     data.add_tfm(preprocessing)

    return data

# Cell
def conv_output(w, ss, ps=None, ks=3):
    ''' image width, strides, pools, kernel sizes '''
    for s,p,k in zip(ss,ps,ks):
        s = s[1] if isinstance(s, tuple) else s
        w = w if w%s == 0 else w + 1
        w = (w - k + 2*p)/s + 1 if p is not None else w/s
    return int(w)

# Cell
class CNN(nn.Module):
    def __init__(self, d_model, cnn_layers, kernels, strides, channels, padding, nc=3):
        super().__init__()
        layers = []
        for layer,i,o,k,s,p in zip(cnn_layers, [nc] + channels[:-1], channels, kernels, strides, padding):
            layers.append( layer(ni=i, nf=o, ks=k, stride=s, padding=p) )
        self.cnn = nn.Sequential(*layers)
        b,c,h,w = self.cnn(torch.zeros(1,3,48,128)).shape
        self.out = nn.Linear(h*c, d_model)
        print('CNN output = h:{} c:{}'.format(h,c))

    def forward(self, x):
        x = self.cnn(x).permute(0,3,1,2)
        b,w,c,h = x.shape
        return self.out(x.view(b,w,-1)) # [b,c,h,w]

# Cell
import revtorch as rv

def RevConv(ni, nf, ks, stride, padding):
    assert ni == nf and stride == 1
    f_func = conv_layer(ni//2, nf//2, ks, stride=stride, padding=padding)
    g_func = conv_layer(ni//2, nf//2, ks, stride=stride, padding=padding)
    layers = nn.ModuleList([rv.ReversibleBlock(f_func, g_func)])
    return rv.ReversibleSequence(layers, eagerly_discard_variables = True)

# Cell
def get_normal_cnn(dx=1):
    strides = [2, 1, (2,1), 1, (2,1), 1, (2,1), 1]
    channels = [int(c*dx) for c in [64, 64, 128, 128, 256, 256, 512, 512]]
    cnn_layers = [conv_layer] * len(strides)
    kernels = [3] * len(strides)
    padding = [None] * len(strides) # None - out size doesnt change
    return cnn_layers, channels, kernels, strides, padding

# Cell
def get_partially_rev_cnn(dx=1):
    strides = [2, 1, (2,1), 1, (2,1), 1, (2,1), 1]
    channels = [int(c*dx) for c in [64, 64, 128, 128, 256, 256, 512, 512]]
    cnn_layers = [conv_layer, RevConv] * (len(strides)//2)
    kernels = [3] * len(strides)
    padding = [None] * len(strides) # None - out size doesnt change
    return cnn_layers, channels, kernels, strides, padding

# Cell
class CRNN(nn.Module):

    def __init__(self, fclass=11, nclass=12, nc=3, d_model=512, rnn_hidden=256, bidirectional=True):
        super().__init__() # fclass - font classes, nclass - char classes

        cnn_layers, self.channels, self.kernels, self.strides, self.padding = get_partially_rev_cnn(dx=1/2)
        self.cnn = CNN(d_model, cnn_layers, self.kernels, self.strides, self.channels, self.padding, nc=nc)

        # font prediction
        h,w = 2,d_model
        self.adaptive_pool = nn.AdaptiveAvgPool2d([h,w]) # this([h,w])([B,H,W]) -> [B,h,w]
        f_model = 2 # font embedding
        self.font_ff = nn.Sequential(nn.Linear(h*w, fclass*f_model), nn.ReLU())
        self.font_out = nn.Linear(f_model, 1)
#         self.font_emb = nn.Linear(f_model, d_model)

        # char prediction
        self.rnn = nn.LSTM(d_model, rnn_hidden, bidirectional=bidirectional)
        mult = 1 if not bidirectional else 2
        d_model = rnn_hidden * mult

        self.out = nn.Linear(d_model, nclass)

        self.nclass, self.d_model, self.fclass, self.f_model = nclass, d_model, fclass, f_model

    def forward(self, x):
        ''' [b,c,h,w], [b,s_d] '''
        b,c,h,w = x.shape
        x = self.cnn(x) # [b,w,512]
        f = self.adaptive_pool(x).view(b,-1) # [b,h_a x w_a] (_a = adaptive pool params)
        f_enc = self.font_ff(f).view(-1, self.fclass, self.f_model)
        f_out = self.font_out(f_enc).view(-1, self.fclass)
#         f_emb = self.font_emb(f_enc) # [b,f,512]
        x, _ = self.rnn(x)
        return f_out, self.out(x)

# Cell
image_width2seq_len = lambda w: conv_output(w, crnn.strides, crnn.padding, crnn.kernels)

# Cell
class CTCFontLoss(nn.Module):
    def __init__(self, ctc_pad_idx=0):
        super().__init__()
        self.ctc_loss = nn.CTCLoss(blank=ctc_pad_idx, reduction='mean', zero_infinity=True)
        self.metric_names = ['ctc_loss', 'font_loss']

    def _ctc_loss(self, y_pred, y_true, y_pred_len, y_true_len):
        # y_pred: [b,s_e,c], y_true: [[s_d], [s_d], ...], lengths: [b]
        b, s_e, c = y_pred.shape
#         print([len(x) for x in y_true], [len(x) for x in y_pred])
        y_true = torch.cat(y_true, axis=0) # [b*s_d]
        y_pred = y_pred.log_softmax(axis=2).permute(1,0,2) # [ s_e, b, c ]
        torch.backends.cudnn.enabled = False
        loss = self.ctc_loss(y_pred, y_true, y_pred_len, y_true_len)
        torch.backends.cudnn.enabled = True
        return loss

    def _font_loss(self, y_pred, y_true): # [b,c], [b]
        return nn.CrossEntropyLoss()(y_pred, y_true)

    def forward(self, nn_output, font_true, y_true, y_pred_len, y_true_len):
        font_pred, y_pred = nn_output
        ctc = self._ctc_loss(y_pred, y_true, y_pred_len, y_true_len)
        font = self._font_loss(font_pred, font_true)
        self.metrics = dict(zip(self.metric_names, [ctc, font]))
        return ctc + font

# Cell
from fastai.callbacks import LossMetrics

class AddLossMetrics(LossMetrics):
    def on_batch_end(self, last_target, train, **kwargs):
        "Update the metrics if not `train`"
        if train: return
        bs = last_target[0].size(0) # CHANGED
        for name in filter(lambda n: n.endswith('loss'), self.names):
            self.metrics[name] += bs * self.learn.loss_func.metrics[name].detach().cpu()
        self.nums += bs

# Cell
def wer(s1,s2):
    ''' s1 - true text, s2 - pred text '''
    d = np.zeros([len(s1)+1,len(s2)+1])
    d[:,0] = np.arange(len(s1)+1)
    d[0,:] = np.arange(len(s2)+1)

    for j in range(1,len(s2)+1):
        for i in range(1,len(s1)+1):
            if s1[i-1] == s2[j-1]: d[i,j] = d[i-1,j-1]
            else: d[i,j] = min(d[i-1,j]+1, d[i,j-1]+1, d[i-1,j-1]+1)

    return d[-1,-1]/len(s1)

word_error = wer( 'black frog jumped away'.split(' '), 'black frog jumped awayyy'.split(' ') )
char_error = wer( 'black frog jumped away', 'black frog jumped awayyy' )
char_error, word_error

# Cell
def decode_true(texts): return texts

# Cell
class WordErrorRate(LearnerCallback):
    _order=-20 # Needs to run before the recorder
    def __init__(self, learner, decode_pred_func=decode_ctc, decode_true_func=decode_true):
        super().__init__(learner)
        self.classes = learner.data.classes
        self.decode_pred, self.decode_true = decode_pred_func, decode_true_func
        if not hasattr(self.learn.recorder, 'names'):
            self.learn.recorder.add_metric_names(['cer', 'wer'])

    def on_epoch_begin(self, **kwargs):
        self.wer, self.cer, self.total = 0, 0, 0

    def on_batch_end(self, last_output, last_target, train, **kwargs):
        if train: return
        font_pred, y_pred = last_output
        font_true, y_true, *_ = last_target
        y_pred = y_pred.argmax(-1)
        y_pred = self.decode_pred(y_pred)
        y_true = self.decode_true(y_true)
        for yp, yt in zip(y_pred, y_true):
            self.total += 1
            if yp.shape == torch.Size([]): continue
            yt_text = ''.join([self.classes[i] for i in yt])
            yp_text = ''.join([self.classes[i] for i in yp])
            self.wer += wer(yt_text.split(' '), yp_text.split(' '))
            self.cer += wer(yt_text, yp_text)

    def on_epoch_end(self, last_metrics, **kwargs):
        if self.total == 0: return {'last_metrics': last_metrics}
        metrics = [tensor(self.cer/self.total), tensor(self.wer/self.total)]
        return {'last_metrics': [last_metrics[0]] + metrics + last_metrics[1:]}